{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NonLinear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of least squares is a standard approach used in statistical regression analysis, and its most important application is in data fitting -> used to predict behaviour of dependent variables. \n",
    "\n",
    "In practice, we often have to determine unknown parameters of a mathematical model to fit the data given, and usually the number of measurements given is much greater than the number of unknowns -> this is known as an **overdetermined system**.\n",
    "\n",
    "![overdetermined_system.png](img/overdetermined_system.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in a previous group presentation on Linear least squares (LLS), in least squares problems we need to solve this overdetermined system but generally there is no unique solution, so we use optimization to approximate a solution.\n",
    "\n",
    "There are many similarities between LLS and NLLS, but also some significant differences as well.\n",
    "\n",
    "LLS has linear parameters, which can be reduced to a linear algebra problem to be solved analytically.  If the regression model doesn't follow the rules for a linear model, then it is nonlinear.\n",
    "\n",
    "One benefit of using Nonlinear least squares is that it extends linear least squares for a much larger and general class of functions.  Linear models do not describe processes that asymptote very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples you may be familiar with that illustrate the diversity of non-linear models: $\\theta$ for parameters, $X$s for independent variable.\n",
    "\n",
    "**Exponential growth model**: $\\theta_1 * X^{\\theta_2}$ -> example on how to implement later\n",
    "\n",
    "![exponential_nonlinear_model.png](img/exponential_nonlinear_model.png)\n",
    "\n",
    "\n",
    "\n",
    "**Weibull growth model**: $\\theta_1 + (\\theta_2 - \\theta_1) * \\exp(-\\theta_3 * X^{\\theta_4})$\n",
    "\n",
    "![weibull_growth_model.png](img/weibull_growth_model.png)\n",
    "\n",
    "\n",
    "**Fourier series model**: $\\theta_1 * \\cos(X + \\theta_4) + \\theta_2 * \\cos(2 * X + \\theta_4) + \\theta_3$\n",
    "\n",
    "![fourier_nonlinear_model.png](img/fourier_nonlinear_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to LLS, in Non-linear least squares (NLLS), we want to minimize the sum of the square of residuals, which is a type of loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective function: Find $a \\in \\mathbb{R}^n$ that minimizes:**\n",
    "$$\n",
    "|| r(a) ||^2 = \\sum_{i-1}^{m} r_i(a)^2\n",
    "$$\n",
    "\n",
    "where $r: \\mathbb{R}^n \\to \\mathbb{R}^m$, $r(a)$ is a vector of residuals, model is of the form $y \\sim f(x, a)$, residuals is the observed value minus predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing The Objective Function\n",
    "\n",
    "Now, there are multiple ways we can try and reduce the objective function for least squares problems, why choose one over another?\n",
    "\n",
    "We know that the minimum value occurs when the gradient is zero.\n",
    "\n",
    "Recall, the group presentation on LLS solved their problems explicitly because they have a closed form expression for the gradient of the objective function.  However, this can be extremely expensive to compute and not always practical:\n",
    "\n",
    "$ Ax \\approx b $ has the solution $x = (A^T A)^{-1} A^T b$ -> imagine $X^T X$ is a dense 100,000 x 100,000 element matrix.  Calculating this would require $1 x 10^{10}$ floating numbers (at 8 bytes per number, this is 80 gb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way in NLLS, there is **no closed form solution**.\n",
    "\n",
    "Since we cannot analytically solve the model, we use iterative optimization procedures to compute the parameter estimates.  Unfortunately the downside to using iterative procedures means it is more costly and requires user to provide starting values for unknown parameters that are reasonably close, otherwise it may not converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Non-Convex Optimization\n",
    "\n",
    "Some iterative methods to find the minimum of the objective function include the following:  Gradient Descent, Newton's Method, Stochastic Gradient Descent.  Today our presentation will be focusing on the Gauss-Newton algorithm (GN) which is a modification of Newton's method, but you may be wondering why GN?\n",
    "\n",
    "**Gradient Descent**: $ a^{t+1} = a^t - \\alpha \\bigtriangledown f(a^t) $\n",
    "\n",
    "Pros: Especially for LLS, gradient descent never explicitly forms the matrix product $X^T X$ and would be much more computationally efficient than using the closed form solution to the least squares problem.\n",
    "\n",
    "Cons: Inefficient for large data sets.  For example, if we used 23000 genes to predict if someone had a disease, and data from 5000 samples?  We need to calculate 5000 terms for each of the 23000 derivatives, in other words calculate 115,000,000 terms for each step.  It can also be difficult to define a proper step size.\n",
    "\n",
    "**Newton's Method**: $ a^{t+1} = a^t - \\bigtriangledown f(a^t) \\bigtriangledown^2 f(a^t)^{-1} $\n",
    "\n",
    "Pros: Solves the inefficiency issue of gradient descent, since it rescales the gradients in each direction with the inverse of corresponding eigenvalues of the hessian.\n",
    "\n",
    "Cons: It can end up moving in the wrong direction (negative eigenvalues).  The second derivative if often complicated or intractable, requiring lots of computation.  It also attracts to saddle points in multivariable optimization.\n",
    "\n",
    "**Stochastic Gradient Descent**: Gradient descent but picking one random point to compute derivatives instead of all of the points.\n",
    "\n",
    "Pros: Computational less expensive, frequent updates create oscillations which can be helpful for getting out of local minimums.\n",
    "\n",
    "Cons: Can go in the wrong direction, lose benefits of vectorization since we process one observation per time, step size can be difficult to determine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
