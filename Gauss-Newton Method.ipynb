{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Gauss-Newton Method\n",
    "\n",
    "Gauss-Newton Method is an *approximation* of Newton's Method for non-linear least squares. The following is the recurrence relation for Newton's Method that is calculated at each step. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "x^{t+1} &= x^{t} - \\nabla^{2}f(x^t)^{-1}\\nabla f(x^t) \\\\\n",
    "&= x^{t} - \\boldsymbol{H}^{-1}\\boldsymbol{g}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Some maybe wondering why the second derivative appears in Newton's method in the first place. Recall that Newton's method is a root-finding algorithm. In a least-squares problem we aren't trying to determine the root of the objective function $f(x)$. Rather we are trying to determine the root of the gradient of the objective function i.e. $\\nabla f(x) = 0$.\n",
    "\n",
    "The problem with the above expression is that the Hessian is often computationally taxing to calculate. \n",
    "\n",
    "## Gauss-Newton Method\n",
    "\n",
    "Gauss-Newton Method makes the following assumptions in order to *approximate* the Hessian and make it more manageable to calculate.\n",
    "\n",
    "1. The Objective Function is a **sum of squares** \n",
    "    $$\n",
    "f(x) = \\sum_{i=1}^{m} r_{i}(x)^2 \\\\\n",
    "$$\n",
    "\n",
    "    The gradient and the hessian of sum of squares function can be expressed as follows.\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "\\nabla f_{j}(x) &= 2\\sum_{i=1}^{m} \\frac{\\partial r_{i}}{\\partial x_j} \\\\\n",
    "\\nabla^{2} f_{jk}(x) &= 2\\sum_{i=1}^{m} \\left(\\frac{\\partial r_{i}}{\\partial x_j}\\frac{\\partial r_{i}}{\\partial x_k} + r_{i}\\frac{\\partial^2 r_{i}}{\\partial x_j \\partial x_k}\\right) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "2. **Drop the second derivative** term in the above expression for the Hessian\n",
    "\n",
    "    In order to remove the second derivative term from the expression, we assume that the first derivative term dominates.\n",
    "\n",
    "    $$\n",
    "\\left\\lvert \\frac{\\partial r_{i}}{\\partial x_j}\\frac{\\partial r_{i}}{\\partial x_k}  \\right\\rvert >> \\left\\lvert r_{i}\\frac{\\partial^2 r_{i}}{\\partial x_j \\partial x_k}\\right\\rvert\n",
    "$$\n",
    "\n",
    "    Applying the above assumption, we can re-write the expression for the gradient and the hessian in matrix notation.\n",
    "\n",
    "    $$\\begin{align}\n",
    "\\boldsymbol{g} &= 2J^{T}_{r}r \\\\\n",
    "\\boldsymbol{H} &\\approx 2J_{r}^{T}J_{r}\n",
    "\\end{align}$$\n",
    "\n",
    "    Substituting the two expressions into the recurrence relation results in the following.\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "x^{t+1} &= x^{t} - \\boldsymbol{H}^{-1}\\boldsymbol{g} \\\\\n",
    "&\\approx x^{t} - \\boldsymbol{H_{approx}}^{-1}\\boldsymbol{g} \\\\\n",
    "&= x^{t} - (2J_{r}^{T}J_{r})^{-1}(2J^{T}_{r}r(x^t)) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "    $$\n",
    "x^{t+1} = x^{t} - (J_{r}^{T}J_{r})^{-1}J^{T}_{r}r(x^t)\n",
    "$$\n",
    "\n",
    "    Note: $J_{r}^{T}J_{r}$ has to be invertible for above equation to hold. This condition implies that $J$ needs to have full column rank and that $\\text{# of equation} \\geq \\text{# of variables}$ i.e. $m \\geq n$.\n",
    "\n",
    "We can notice here that the second term in the recurrence relation above is equivalent to the solution to the *linear least squares* problem $\\|Ax-b\\|^2$ where $b = r(x^t)$ and $A = J_{r}$. \n",
    "\n",
    "So, approximating the Hessian by the means outlined above is equivalent to solving a linear least squares problem at each step. We can visualize this process for the case where $x \\in R$ (Explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gauss-newton](https://i.stack.imgur.com/gdJ3v.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "Since Gauss-Newton Method is an approximation of Newton's method, convergence properties are similar if the assumptions made are well satisfied.\n",
    "$$\n",
    "\\left\\lvert r_{i}\\frac{\\partial^2 r_{i}}{\\partial x_j \\partial x_k}\\right\\rvert\n",
    "$$\n",
    "\n",
    "Local convergence to $x^k$ is quadratic much like Newton's method if the following two conditions are met. Otherwise, convergence is linear.\n",
    "\n",
    "* $r_{i}(x^k)$ is small\n",
    "* $\\frac{\\partial^2 r_{i}}{\\partial x_j \\partial x_k}$ is small i.e. $r_{i}(x^k)$ is nearly affine\n",
    "\n",
    "So, due to these factors, the rate of convergence is dependent on the initial position and the nature of the function being operated on. These factors determine the effectiveness of the Hessian approximation.\n",
    "\n",
    "Also, local convergence isn't guaranteed. Provide reasoning.\n",
    "\n",
    "\n",
    "\n",
    "What are you giving up?\n",
    "* Newton's Method is more general\n",
    "* Important thing to remember: $J_r^TJ_r$ need to be invertible! m>n \n",
    "\n",
    "**Question for Christoph**\n",
    "Would it be fair to assume that given these more \"strict\" convergence criterion, most of the benefits/downsides of Newton's Method still applies to Gauss-Newton Method? Namely, the re-scaling of the gradient to determine the step size, \"faster\" convergence. Problem with saddle points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r(x) \\approx r(x^k) + J(x^k)(x-x^k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence of a Gauss-Newton Method**\n",
    "\n",
    "Due to the approximations mentioned above, local convergence isn't guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges of Gauss-Newton Method**\n",
    "* large scale systems: exactly solving LLS at each step is too expensive.\n",
    "\n",
    "\n",
    "Comparison with Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent is OK, but the iterates only converge in a statistical sense and not to high tolerance, so it is comparing apples to oranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://see.stanford.edu/materials/lsoeldsee263/07-ls-reg.pdf\n",
    "2. http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf\n",
    "3. https://friedlander.io/19T2-406/notes/Non-linear_LS/\n",
    "4. https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning\n",
    "5. \"Numerical optimization\" Nocedal and Wright.\n",
    "6. https://www.cs.ubc.ca/labs/lci/mlrg/slides/non_convex_optimization.pdf\n",
    "7. https://stats.stackexchange.com/questions/327251/gradient-descent-on-non-convex-functions/328500#328500\n",
    "\n",
    "https://math.stackexchange.com/questions/1840801/why-is-ata-invertible-if-a-has-independent-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
